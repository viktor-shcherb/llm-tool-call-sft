data:
  load_module: "car_sales"
  dataset_name: "Salesteq/car-sales-convos"
  timezone: "Europe/Zurich"
  sysprompt_path: "data/sysprompt.md"
  tools_path: "data/tool_calls.json"

  tokenizer: "viktoroo/SmolLM2-360M-Tools"
  max_context_length: 8192
  drop_oversized: true

model:
  base_model_name: "viktoroo/SmolLM2-360M-Tools"
  torch_dtype: "bfloat16"
  pad_token_fallback: "eos"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: "none"
  task_type: "CAUSAL_LM"

train:
  # basic
  output_dir: "checkpoints/car-sales-sft"
  seed: 42

  # logging
  report_to: "wandb"

  # core trainer args
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  gradient_checkpointing: false
  num_train_epochs: 3

  learning_rate: 0.0002
  weight_decay: 0.0
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1

  bf16: true
  fp16: false

  ddp_find_unused_parameters: false

  logging_steps: 5
  eval_strategy: "steps"
  eval_steps: 20
  save_strategy: "epoch"
  save_total_limit: 2
  overwrite_output_dir: false
  remove_unused_columns: false

  # hub
  push_to_hub: true
  hub_model_id: "Salesteq/SmolLM2-360M-CarSales"
  hub_private_repo: true

  # --- FSDP ---
  fsdp: "full_shard auto_wrap"
  fsdp_config:
    fsdp_transformer_layer_cls_to_wrap: "LlamaDecoderLayer"
    use_orig_params: true
    sync_module_states: true
    forward_prefetch: true
    state_dict_type: "full"3

eval:
  max_new_tokens_eval: 256
  temperature: 0.0
